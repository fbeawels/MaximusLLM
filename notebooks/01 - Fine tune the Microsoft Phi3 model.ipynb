{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Fine tune the Microsoft Phi3 model to create MaximusLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This notebook aims to fine tune the Phi3 Mini Instruct 128k model from Microsoft to create a custom model named MaximusLLM.\n",
    "The fine-tuning process will be conducted using the Hugging Face Transformers library and the PEFT library for efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Import the required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following libraries are requiredered to fine-tune the model:\n",
    "\n",
    "- ***sys***: Provides access to some variables used or maintained by the Python interpreter and to functions that interact strongly with the interpreter.\n",
    "- ***logging***: Allows you to emit messages to a log file or to the system console.\n",
    "- ***datasets***: Provides a simple and efficient way to load, preprocess, and share datasets.\n",
    "- ***huggingface_hub***: Offers a command-line interface to the Hugging Face Hub, allowing you to upload and download models, datasets, and other artifacts.\n",
    "- ***peft***: Enables parameter-efficient fine-tuning of large language models, reducing the computational resources required for training.\n",
    "- ***torch***: A popular machine learning library that provides a wide range of functionalities for building and training neural networks.\n",
    "- ***transformers***: A library for state-of-the-art natural language processing (NLP) models, providing easy-to-use interfaces for tasks such as text classification, translation, and question answering.\n",
    "- ***trl***: A library for training reinforcement learning agents in NLP tasks, such as dialogue generation and sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import dotenv\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import wandb\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft.utils.save_and_load import get_peft_model_state_dict\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the environment variables containing the various tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set the CUDA_HOME varaible for the NVidia GPU libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_HOME\"]=\"/usr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Define the fine tuning parameters and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a configuration dictionary for fine-tuning the Microsoft Phi3 model. It includes settings for training, evaluation, learning rate, logging, and saving checkpoints. The training uses bf16 precision, a cosine learning rate scheduler, and gradient checkpointing for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"bf16\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 5.0e-06,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 20,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./checkpoint_dir\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a configuration dictionary for fine-tuning a Microsoft Phi3 model using Parameter-Efficient Fine-Tuning (PEFT) with Low-Rank Adaptation (LoRA). It specifies the rank (r) of the update matrices, the scaling factor (lora_alpha), dropout rate, bias handling, task type, target modules for adaptation, and modules to save during the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": \"all-linear\",\n",
    "    \"modules_to_save\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets up training configurations for a model using Hugging Face's Transformers library, including parameters for training and Low-Rank Adaptation (LoRA) fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Setting up the logging for the fine tuining process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logger will be used to log messages at various levels such as debug, info, warning, error, and critical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet sets the logging level for various libraries (datasets, transformers) and the logger to the same level specified in the training configuration. This ensures consistent and appropriate verbosity across the different components of the software, making it easier to debug and understand the execution flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_level = train_conf.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet logs information about the training process, including the process rank, device, and whether distributed training or 16-bit training is being used. It also logs the training/evaluation parameters and PEFT parameters for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:46:55 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n",
      "2024-07-08 20:46:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./checkpoint_dir/runs/Jul08_20-46-55_C3PO,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=20,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.COSINE,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./checkpoint_dir,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./checkpoint_dir,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=1,\n",
      "seed=0,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.2,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "2024-07-08 20:46:55 - INFO - __main__ - PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules='all-linear', lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n"
     ]
    }
   ],
   "source": [
    "logger.warning(\n",
    "    f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
    "    + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
    "logger.info(f\"PEFT parameters {peft_conf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Load the Phi3 model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Microsoft Phi-3 model is a large language model developed by Microsoft. It's a part of the Phi series, which includes models like Phi-1 and Phi-2. The \"Phi-3-mini-128k-instruct\" is a variant of the Phi-3 model that has been fine-tuned on a smaller dataset of 128,000 tokens. This makes it more computationally efficient and faster to train, while still maintaining a good level of performance. The \"-instruct\" in the name suggests that this model has been trained to follow instructions, making it suitable for tasks like text generation and question answering. However, without more specific information, I can't provide a detailed description of its capabilities or limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_model_name = \"microsoft/Phi-3-mini-128k-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets up parameters for loading a Microsoft Phi3 model, including using flash attention for faster computation, using bfloat16 data type for memory efficiency, and not using cache or device mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    #attn_implementation=\"flash_attention_2\",  # loading the model with flash-attention support\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet loads a pre-trained causal language model and its corresponding tokenizer from a specified checkpoint path. This is a common step in fine-tuning a model for a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:46:55,947 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:46:56,073 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:46:56,075 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:46:56 - WARNING - transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3 - `flash-attention` package not found, consider installing for better performance: /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi.\n",
      "2024-07-08 20:46:56 - WARNING - transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3 - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3556] 2024-07-08 20:46:56,382 >> loading weights file model.safetensors from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1531] 2024-07-08 20:46:56,384 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-08 20:46:56,384 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555b251aefdc4da6b9df9c8ff7b6b899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4364] 2024-07-08 20:46:57,559 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-08 20:46:57,560 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-128k-instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:955] 2024-07-08 20:46:57,674 >> loading configuration file generation_config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-08 20:46:57,674 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 20:46:57,947 >> loading file tokenizer.model from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 20:46:57,948 >> loading file tokenizer.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 20:46:57,948 >> loading file added_tokens.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 20:46:57,949 >> loading file special_tokens_map.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 20:46:57,949 >> loading file tokenizer_config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-08 20:46:57,998 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(hd_model_name, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hd_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets the maximum token length to 2048, uses the unknown token for padding to prevent endless generation, adjusts the padding token ID, and sets the padding side to the right for the tokenizer in a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Prepare the training and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a conversation-style example and a tokenizer as input, formats the messages in the example using the chat template provided by the tokenizer, and adds the formatted string to the example dictionary under the key \"text\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(example,tokenizer):\n",
    "    \"\"\"\n",
    "    This function takes a conversation-style example and a tokenizer as input,\n",
    "    formats the messages in the example using the chat template provided by the tokenizer,\n",
    "    and adds the formatted string to the example dictionary under the key \"text\".\n",
    "\n",
    "    Parameters:\n",
    "    example (dict): A dictionary containing a key \"messages\" with a list of messages.\n",
    "    tokenizer (object): An object with a method apply_chat_template that formats a list of messages into a single string.\n",
    "\n",
    "    Returns:\n",
    "    dict: The modified example dictionary with a new key \"text\" containing the formatted string.\n",
    "    \"\"\"\n",
    "    messages = example[\"messages\"]\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code loads a dataset and prints the number of samples in the 'train', 'eval', and 'test' subsets. It shows the dataset's size distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:00 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:00 - INFO - datasets.info - Loading Dataset info from /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset maximo_admin_dataset (/home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:00 - INFO - datasets.builder - Found cached dataset maximo_admin_dataset (/home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:00 - INFO - datasets.info - Loading Dataset info from /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12687"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"awels/maximo_admin_dataset\")\n",
    "len(raw_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching indices mapping at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-303a2a11d701da26.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:00 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-303a2a11d701da26.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching indices mapping at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-c9ef3ef2eecaae0d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:00 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-c9ef3ef2eecaae0d.arrow\n",
      "Train set : 10149\n",
      "Test set : 2538\n"
     ]
    }
   ],
   "source": [
    "# Diviser le dataset d'entraînement en deux splits : train et test\n",
    "train_test_split = raw_dataset[\"train\"].train_test_split(test_size=0.2)  # 20% pour le test, 80% pour l'entraînement\n",
    "\n",
    "# Créer un DatasetDict avec les nouveaux splits\n",
    "new_dataset = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "train_dataset = new_dataset[\"train\"]\n",
    "test_dataset = new_dataset[\"test\"]\n",
    "\n",
    "print (\"Train set : \" + str(len(train_dataset)))\n",
    "print (\"Test set : \" + str(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages'],\n",
       "    num_rows: 10149\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code applies a chat template to the training dataset using a tokenizer, with parallel processing for efficiency, and removes unnecessary columns. Do the same for test datasets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #0 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #1 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #2 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #2 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #3 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #3 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #4 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #4 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #5 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #5 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #6 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #6 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #7 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #7 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #8 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #8 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #9 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Process #9 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spawning 10 processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:01 - INFO - datasets.arrow_dataset - Spawning 10 processes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd4a44e129548ee96cb9ef6a0f07465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train (num_proc=10):   0%|          | 0/10149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00003_of_00010.arrow\n",
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00003_of_00010.arrow\n",
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-01908981a16589d6_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 10 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Concatenating 10 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #0 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #1 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #2 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #2 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #3 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #3 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #4 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #4 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #5 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #5 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #6 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #6 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #7 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #7 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #8 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #8 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #9 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:02 - INFO - datasets.arrow_dataset - Process #9 will write at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spawning 10 processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Spawning 10 processes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411bea10fe9a4e38849429c9fd0318a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to test (num_proc=10):   0%|          | 0/2538 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00000_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00001_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00002_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00003_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00004_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00005_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00006_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00007_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00008_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/franck/.cache/huggingface/datasets/awels___maximo_admin_dataset/default/0.0.0/91b3b084749da7d748f7750b5d350c582dd7ac65/cache-e72d48380e23bb03_00009_of_00010.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 10 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:04 - INFO - datasets.arrow_dataset - Concatenating 10 shards\n"
     ]
    }
   ],
   "source": [
    "column_names = list(train_dataset.features)\n",
    "\n",
    "processed_train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Applying chat template to train\",\n",
    ")\n",
    "\n",
    "processed_test_dataset = test_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Applying chat template to test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Execute the training of the MaximusLLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is using the SFTTrainer (Supervised Fine-Tuning Trainer) to fine-tune the Microsoft Phi3 model. It sets up the model, training configuration, and dataset for training and evaluation. The maximum sequence length is set to 64, and the trainer is configured to pack sequences efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[INFO|training_args.py:2048] 2024-07-08 20:47:04,171 >> PyTorch: setting up devices\n",
      "/home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-2a050f9e3c68d40b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:04 - INFO - datasets.builder - Using custom data configuration default-2a050f9e3c68d40b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/datasets/packaged_modules/generator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:04 - INFO - datasets.info - Loading Dataset Infos from /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/datasets/packaged_modules/generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset generator (/home/franck/.cache/huggingface/datasets/generator/default-2a050f9e3c68d40b/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:04 - INFO - datasets.builder - Generating dataset generator (/home/franck/.cache/huggingface/datasets/generator/default-2a050f9e3c68d40b/0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/franck/.cache/huggingface/datasets/generator/default-2a050f9e3c68d40b/0.0.0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:04 - INFO - datasets.builder - Downloading and preparing dataset generator/default to /home/franck/.cache/huggingface/datasets/generator/default-2a050f9e3c68d40b/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:04 - INFO - datasets.builder - Generating train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207e2f890de9406dba4f2dedab94e20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|tokenization_utils_base.py:3991] 2024-07-08 20:47:05,819 >> Token indices sequence length is longer than the specified maximum sequence length for this model (3546 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:05 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/franck/.cache/huggingface/datasets/generator/default-2a050f9e3c68d40b/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:05 - INFO - datasets.builder - Dataset generator downloaded and prepared to /home/franck/.cache/huggingface/datasets/generator/default-2a050f9e3c68d40b/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b9b796c42d54b53c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:05 - INFO - datasets.builder - Using custom data configuration default-b9b796c42d54b53c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/datasets/packaged_modules/generator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:05 - INFO - datasets.info - Loading Dataset Infos from /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/datasets/packaged_modules/generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset generator (/home/franck/.cache/huggingface/datasets/generator/default-b9b796c42d54b53c/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:05 - INFO - datasets.builder - Generating dataset generator (/home/franck/.cache/huggingface/datasets/generator/default-b9b796c42d54b53c/0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/franck/.cache/huggingface/datasets/generator/default-b9b796c42d54b53c/0.0.0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:05 - INFO - datasets.builder - Downloading and preparing dataset generator/default to /home/franck/.cache/huggingface/datasets/generator/default-b9b796c42d54b53c/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:05 - INFO - datasets.builder - Generating train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4d78855f2749918db41866b70a5d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:06 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/franck/.cache/huggingface/datasets/generator/default-b9b796c42d54b53c/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:06 - INFO - datasets.builder - Dataset generator downloaded and prepared to /home/franck/.cache/huggingface/datasets/generator/default-b9b796c42d54b53c/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:642] 2024-07-08 20:47:07,436 >> Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    peft_config=peft_conf,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_test_dataset,\n",
    "    max_seq_length=64,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code modifies the state_dict method of a PyTorch model to use the get_peft_model_state_dict function from the PEFT library. This is done to fine-tune the Microsoft Phi3 model. If the Torch version is 2 or higher and the platform is not Windows, the model is then compiled for performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_state_dict = model.state_dict\n",
    "\n",
    "def custom_state_dict(self, *args, **kwargs):\n",
    "    return get_peft_model_state_dict(self, original_state_dict(*args, **kwargs))\n",
    "\n",
    "model.state_dict = custom_state_dict.__get__(model, type(model))\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model. PLease note that login to WanDB will be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:08 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfbeawels\u001b[0m (\u001b[33mfbeawels-awels-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/franck/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=os.environ['WANDB_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2128] 2024-07-08 20:47:09,463 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-08 20:47:09,463 >>   Num examples = 12,499\n",
      "[INFO|trainer.py:2130] 2024-07-08 20:47:09,463 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2131] 2024-07-08 20:47:09,464 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2134] 2024-07-08 20:47:09,464 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2135] 2024-07-08 20:47:09,464 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2136] 2024-07-08 20:47:09,464 >>   Total optimization steps = 3,125\n",
      "[INFO|trainer.py:2137] 2024-07-08 20:47:09,466 >>   Number of trainable parameters = 25,165,824\n",
      "[INFO|integration_utils.py:750] 2024-07-08 20:47:09,471 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/franck/Sandbox/03 - Awels Engineering/MaximusLLM/notebooks/wandb/run-20240708_204709-skef71qa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fbeawels-awels-engineering/huggingface/runs/skef71qa' target=\"_blank\">./checkpoint_dir</a></strong> to <a href='https://wandb.ai/fbeawels-awels-engineering/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fbeawels-awels-engineering/huggingface' target=\"_blank\">https://wandb.ai/fbeawels-awels-engineering/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fbeawels-awels-engineering/huggingface/runs/skef71qa' target=\"_blank\">https://wandb.ai/fbeawels-awels-engineering/huggingface/runs/skef71qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1e58a0ddcd4baeb0868ac513c01413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:47:13 - WARNING - transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3 - You are not running the flash-attention implementation, expect numerical differences.\n",
      "{'loss': 6.7887, 'grad_norm': 10.1875, 'learning_rate': 1.6e-07, 'epoch': 0.01}\n",
      "{'loss': 6.5376, 'grad_norm': 6.0, 'learning_rate': 3.2e-07, 'epoch': 0.01}\n",
      "{'loss': 6.2477, 'grad_norm': 12.875, 'learning_rate': 4.800000000000001e-07, 'epoch': 0.02}\n",
      "{'loss': 6.5146, 'grad_norm': 8.0625, 'learning_rate': 6.4e-07, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:47:31,827 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.179, 'grad_norm': 4.8125, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:47:32,054 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:47:32,055 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:47:32,274 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:47:32,275 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:47:32,359 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:47:32,360 >> Special tokens file saved in ./checkpoint_dir/checkpoint-100/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.0984, 'grad_norm': 8.875, 'learning_rate': 9.600000000000001e-07, 'epoch': 0.04}\n",
      "{'loss': 6.2706, 'grad_norm': 8.5, 'learning_rate': 1.12e-06, 'epoch': 0.04}\n",
      "{'loss': 6.2542, 'grad_norm': 3.15625, 'learning_rate': 1.28e-06, 'epoch': 0.05}\n",
      "{'loss': 6.5909, 'grad_norm': 5.9375, 'learning_rate': 1.44e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:47:51,313 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9163, 'grad_norm': 7.0625, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:47:51,542 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:47:51,543 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:47:51,767 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:47:51,769 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:47:51,853 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:47:51,854 >> Special tokens file saved in ./checkpoint_dir/checkpoint-200/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:47:51,979 >> Deleting older checkpoint [checkpoint_dir/checkpoint-100] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7982, 'grad_norm': 4.625, 'learning_rate': 1.76e-06, 'epoch': 0.07}\n",
      "{'loss': 6.391, 'grad_norm': 6.25, 'learning_rate': 1.9200000000000003e-06, 'epoch': 0.08}\n",
      "{'loss': 6.2254, 'grad_norm': 9.0625, 'learning_rate': 2.08e-06, 'epoch': 0.08}\n",
      "{'loss': 5.7959, 'grad_norm': 7.9375, 'learning_rate': 2.24e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:48:10,145 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.2485, 'grad_norm': 4.09375, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:48:10,384 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:48:10,385 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:48:10,609 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:48:10,610 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:48:10,693 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:48:10,693 >> Special tokens file saved in ./checkpoint_dir/checkpoint-300/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:48:10,821 >> Deleting older checkpoint [checkpoint_dir/checkpoint-200] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.682, 'grad_norm': 4.6875, 'learning_rate': 2.56e-06, 'epoch': 0.1}\n",
      "{'loss': 5.4417, 'grad_norm': 5.5625, 'learning_rate': 2.7200000000000002e-06, 'epoch': 0.11}\n",
      "{'loss': 5.1421, 'grad_norm': 5.1875, 'learning_rate': 2.88e-06, 'epoch': 0.12}\n",
      "{'loss': 5.0777, 'grad_norm': 5.1875, 'learning_rate': 3.04e-06, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:48:29,160 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6715, 'grad_norm': 3.8125, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:48:29,393 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:48:29,395 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:48:29,619 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:48:29,621 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:48:29,711 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:48:29,711 >> Special tokens file saved in ./checkpoint_dir/checkpoint-400/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:48:29,837 >> Deleting older checkpoint [checkpoint_dir/checkpoint-300] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.5419, 'grad_norm': 7.34375, 'learning_rate': 3.3600000000000004e-06, 'epoch': 0.13}\n",
      "{'loss': 4.4581, 'grad_norm': 8.125, 'learning_rate': 3.52e-06, 'epoch': 0.14}\n",
      "{'loss': 4.2968, 'grad_norm': 3.453125, 'learning_rate': 3.6800000000000003e-06, 'epoch': 0.15}\n",
      "{'loss': 4.2394, 'grad_norm': 5.84375, 'learning_rate': 3.8400000000000005e-06, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:48:47,539 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.771, 'grad_norm': 3.859375, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:48:47,763 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:48:47,765 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:48:47,984 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:48:47,986 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:48:48,067 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:48:48,068 >> Special tokens file saved in ./checkpoint_dir/checkpoint-500/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:48:48,192 >> Deleting older checkpoint [checkpoint_dir/checkpoint-400] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7784, 'grad_norm': 7.65625, 'learning_rate': 4.16e-06, 'epoch': 0.17}\n",
      "{'loss': 3.8309, 'grad_norm': 6.0625, 'learning_rate': 4.32e-06, 'epoch': 0.17}\n",
      "{'loss': 3.5355, 'grad_norm': 6.84375, 'learning_rate': 4.48e-06, 'epoch': 0.18}\n",
      "{'loss': 3.4408, 'grad_norm': 4.15625, 'learning_rate': 4.6400000000000005e-06, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:49:06,556 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5117, 'grad_norm': 7.78125, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:49:06,788 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:49:06,790 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:49:07,011 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:49:07,013 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:49:07,093 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:49:07,094 >> Special tokens file saved in ./checkpoint_dir/checkpoint-600/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:49:07,217 >> Deleting older checkpoint [checkpoint_dir/checkpoint-500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5405, 'grad_norm': 5.625, 'learning_rate': 4.960000000000001e-06, 'epoch': 0.2}\n",
      "{'loss': 3.1879, 'grad_norm': 6.1875, 'learning_rate': 4.999555880952023e-06, 'epoch': 0.2}\n",
      "{'loss': 3.2989, 'grad_norm': 4.84375, 'learning_rate': 4.997582336695312e-06, 'epoch': 0.21}\n",
      "{'loss': 3.1321, 'grad_norm': 2.796875, 'learning_rate': 4.9940312659030635e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:49:25,756 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1179, 'grad_norm': 3.734375, 'learning_rate': 4.9889049115077e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:49:26,000 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:49:26,002 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:49:26,222 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:49:26,224 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:49:26,314 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:49:26,315 >> Special tokens file saved in ./checkpoint_dir/checkpoint-700/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:49:26,440 >> Deleting older checkpoint [checkpoint_dir/checkpoint-600] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0694, 'grad_norm': 2.484375, 'learning_rate': 4.9822065114245345e-06, 'epoch': 0.23}\n",
      "{'loss': 3.1717, 'grad_norm': 2.8125, 'learning_rate': 4.973940296506628e-06, 'epoch': 0.24}\n",
      "{'loss': 2.8742, 'grad_norm': 3.421875, 'learning_rate': 4.964111487872496e-06, 'epoch': 0.24}\n",
      "{'loss': 2.7917, 'grad_norm': 3.78125, 'learning_rate': 4.952726293608335e-06, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:49:44,137 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9103, 'grad_norm': 2.65625, 'learning_rate': 4.939791904846869e-06, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:49:44,368 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:49:44,370 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:49:44,595 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:49:44,596 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:49:44,687 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:49:44,688 >> Special tokens file saved in ./checkpoint_dir/checkpoint-800/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:49:44,813 >> Deleting older checkpoint [checkpoint_dir/checkpoint-700] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8098, 'grad_norm': 5.34375, 'learning_rate': 4.925316491225265e-06, 'epoch': 0.26}\n",
      "{'loss': 2.8852, 'grad_norm': 4.875, 'learning_rate': 4.909309195725025e-06, 'epoch': 0.27}\n",
      "{'loss': 2.8348, 'grad_norm': 3.421875, 'learning_rate': 4.891780128897077e-06, 'epoch': 0.28}\n",
      "{'loss': 2.6624, 'grad_norm': 2.828125, 'learning_rate': 4.8727403624757365e-06, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:50:03,056 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6674, 'grad_norm': 1.9140625, 'learning_rate': 4.852201922385564e-06, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:50:03,287 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:50:03,288 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:50:03,535 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:50:03,536 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:50:03,627 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:50:03,628 >> Special tokens file saved in ./checkpoint_dir/checkpoint-900/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:50:03,755 >> Deleting older checkpoint [checkpoint_dir/checkpoint-800] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7929, 'grad_norm': 2.03125, 'learning_rate': 4.830177781145528e-06, 'epoch': 0.29}\n",
      "{'loss': 2.7226, 'grad_norm': 3.0625, 'learning_rate': 4.8066818496752875e-06, 'epoch': 0.3}\n",
      "{'loss': 2.6794, 'grad_norm': 2.46875, 'learning_rate': 4.781728968508757e-06, 'epoch': 0.31}\n",
      "{'loss': 2.54, 'grad_norm': 1.4765625, 'learning_rate': 4.755334898420507e-06, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:50:22,124 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7562, 'grad_norm': 1.8046875, 'learning_rate': 4.72751631047092e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:50:22,383 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:50:22,385 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:50:22,614 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:50:22,615 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:50:22,693 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:50:22,694 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:50:22,817 >> Deleting older checkpoint [checkpoint_dir/checkpoint-900] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.66, 'grad_norm': 1.9453125, 'learning_rate': 4.6982907754763905e-06, 'epoch': 0.33}\n",
      "{'loss': 2.5699, 'grad_norm': 3.03125, 'learning_rate': 4.667676752911225e-06, 'epoch': 0.33}\n",
      "{'loss': 2.7018, 'grad_norm': 1.8984375, 'learning_rate': 4.635693579248238e-06, 'epoch': 0.34}\n",
      "{'loss': 2.7083, 'grad_norm': 2.453125, 'learning_rate': 4.6023614557454235e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:50:41,299 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6462, 'grad_norm': 4.03125, 'learning_rate': 4.567701435686405e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:50:41,526 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:50:41,528 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:50:41,751 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:50:41,752 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:50:41,833 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:50:41,833 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1100/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:50:41,958 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5091, 'grad_norm': 1.90625, 'learning_rate': 4.531735411082735e-06, 'epoch': 0.36}\n",
      "{'loss': 2.677, 'grad_norm': 2.046875, 'learning_rate': 4.494486098846428e-06, 'epoch': 0.36}\n",
      "{'loss': 2.4764, 'grad_norm': 1.4140625, 'learning_rate': 4.455977026441471e-06, 'epoch': 0.37}\n",
      "{'loss': 2.5442, 'grad_norm': 2.40625, 'learning_rate': 4.416232517023375e-06, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:51:00,339 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4671, 'grad_norm': 1.328125, 'learning_rate': 4.3752776740761495e-06, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:51:00,563 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:51:00,564 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:51:00,797 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:51:00,798 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:51:00,888 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:51:00,889 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1200/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:51:01,016 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1100] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4465, 'grad_norm': 2.515625, 'learning_rate': 4.333138365556401e-06, 'epoch': 0.39}\n",
      "{'loss': 2.5087, 'grad_norm': 1.859375, 'learning_rate': 4.289841207554578e-06, 'epoch': 0.4}\n",
      "{'loss': 2.4803, 'grad_norm': 2.015625, 'learning_rate': 4.245413547483682e-06, 'epoch': 0.4}\n",
      "{'loss': 2.5494, 'grad_norm': 1.4453125, 'learning_rate': 4.199883446806048e-06, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:51:19,285 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.485, 'grad_norm': 2.296875, 'learning_rate': 4.15327966330913e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:51:19,547 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:51:19,548 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:51:19,805 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:51:19,806 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:51:19,887 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:51:19,888 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1300/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:51:20,011 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1200] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4529, 'grad_norm': 1.6640625, 'learning_rate': 4.1056316329414616e-06, 'epoch': 0.42}\n",
      "{'loss': 2.4682, 'grad_norm': 2.4375, 'learning_rate': 4.056969451220282e-06, 'epoch': 0.43}\n",
      "{'loss': 2.6337, 'grad_norm': 1.25, 'learning_rate': 4.007323854222562e-06, 'epoch': 0.44}\n",
      "{'loss': 2.4067, 'grad_norm': 2.96875, 'learning_rate': 3.956726199171441e-06, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:51:37,591 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3615, 'grad_norm': 2.9375, 'learning_rate': 3.905208444630326e-06, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:51:37,829 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:51:37,831 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:51:38,061 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:51:38,062 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:51:38,156 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:51:38,156 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1400/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:51:38,287 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1300] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3905, 'grad_norm': 3.4375, 'learning_rate': 3.85280313031719e-06, 'epoch': 0.45}\n",
      "{'loss': 2.3238, 'grad_norm': 1.7265625, 'learning_rate': 3.7995433565517737e-06, 'epoch': 0.46}\n",
      "{'loss': 2.4784, 'grad_norm': 1.953125, 'learning_rate': 3.7454627633487274e-06, 'epoch': 0.47}\n",
      "{'loss': 2.4399, 'grad_norm': 2.078125, 'learning_rate': 3.6905955091698483e-06, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:51:56,473 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4648, 'grad_norm': 1.7734375, 'learning_rate': 3.634976249348867e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:51:56,744 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:51:56,745 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:51:56,972 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:51:56,973 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:51:57,065 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:51:57,065 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:51:57,193 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1400] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4148, 'grad_norm': 1.6796875, 'learning_rate': 3.578640114202398e-06, 'epoch': 0.49}\n",
      "{'loss': 2.4486, 'grad_norm': 3.0625, 'learning_rate': 3.521622686840873e-06, 'epoch': 0.49}\n",
      "{'loss': 2.3974, 'grad_norm': 1.71875, 'learning_rate': 3.463959980693492e-06, 'epoch': 0.5}\n",
      "{'loss': 2.459, 'grad_norm': 2.078125, 'learning_rate': 3.4056884167613646e-06, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:52:14,775 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3272, 'grad_norm': 3.625, 'learning_rate': 3.346844800613229e-06, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:52:15,044 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:52:15,046 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:52:15,303 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:52:15,304 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:52:15,395 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:52:15,396 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1600/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:52:15,521 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5175, 'grad_norm': 2.578125, 'learning_rate': 3.287466299138262e-06, 'epoch': 0.52}\n",
      "{'loss': 2.4285, 'grad_norm': 1.6875, 'learning_rate': 3.2275904170706795e-06, 'epoch': 0.52}\n",
      "{'loss': 2.3712, 'grad_norm': 1.9375, 'learning_rate': 3.1672549733009396e-06, 'epoch': 0.53}\n",
      "{'loss': 2.3639, 'grad_norm': 1.59375, 'learning_rate': 3.106498076988519e-06, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:52:33,371 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3723, 'grad_norm': 1.6875, 'learning_rate': 3.045358103491357e-06, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:52:33,602 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:52:33,604 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:52:33,828 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:52:33,829 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:52:33,909 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:52:33,910 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1700/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:52:34,035 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1600] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.521, 'grad_norm': 3.265625, 'learning_rate': 2.9838736701271514e-06, 'epoch': 0.55}\n",
      "{'loss': 2.4792, 'grad_norm': 2.640625, 'learning_rate': 2.9220836117818346e-06, 'epoch': 0.56}\n",
      "{'loss': 2.3851, 'grad_norm': 1.8671875, 'learning_rate': 2.8600269563806304e-06, 'epoch': 0.56}\n",
      "{'loss': 2.4396, 'grad_norm': 3.265625, 'learning_rate': 2.797742900237175e-06, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:52:51,582 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3736, 'grad_norm': 1.890625, 'learning_rate': 2.7352707832962865e-06, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:52:51,817 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:52:51,819 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:52:52,050 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:52:52,052 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:52:52,142 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:52:52,143 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1800/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:52:52,270 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1700] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3766, 'grad_norm': 1.984375, 'learning_rate': 2.6726500642860155e-06, 'epoch': 0.58}\n",
      "{'loss': 2.5268, 'grad_norm': 2.265625, 'learning_rate': 2.6099202957946624e-06, 'epoch': 0.59}\n",
      "{'loss': 2.4639, 'grad_norm': 2.515625, 'learning_rate': 2.5471210992885207e-06, 'epoch': 0.6}\n",
      "{'loss': 2.4178, 'grad_norm': 1.609375, 'learning_rate': 2.484292140086103e-06, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:53:09,174 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-1900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5282, 'grad_norm': 2.75, 'learning_rate': 2.4214731023046795e-06, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:53:09,400 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:53:09,401 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:53:09,622 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:53:09,624 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:53:09,728 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-1900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:53:09,729 >> Special tokens file saved in ./checkpoint_dir/checkpoint-1900/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:53:09,867 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1800] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4304, 'grad_norm': 1.109375, 'learning_rate': 2.358703663794939e-06, 'epoch': 0.61}\n",
      "{'loss': 2.3878, 'grad_norm': 1.8515625, 'learning_rate': 2.2960234710796065e-06, 'epoch': 0.62}\n",
      "{'loss': 2.4993, 'grad_norm': 1.96875, 'learning_rate': 2.2334721143118506e-06, 'epoch': 0.63}\n",
      "{'loss': 2.3776, 'grad_norm': 1.765625, 'learning_rate': 2.171089102269294e-06, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:53:26,886 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2795, 'grad_norm': 1.5546875, 'learning_rate': 2.1089138373994226e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:53:27,118 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:53:27,119 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:53:27,358 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:53:27,359 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:53:27,440 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:53:27,441 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:53:27,568 >> Deleting older checkpoint [checkpoint_dir/checkpoint-1900] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4203, 'grad_norm': 1.90625, 'learning_rate': 2.0469855909321565e-06, 'epoch': 0.65}\n",
      "{'loss': 2.2409, 'grad_norm': 2.109375, 'learning_rate': 1.9853434780752977e-06, 'epoch': 0.65}\n",
      "{'loss': 2.3798, 'grad_norm': 2.0625, 'learning_rate': 1.9240264333085247e-06, 'epoch': 0.66}\n",
      "{'loss': 2.3994, 'grad_norm': 2.125, 'learning_rate': 1.8630731857915451e-06, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:53:44,912 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2968, 'grad_norm': 2.25, 'learning_rate': 1.8025222349019273e-06, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:53:45,138 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:53:45,139 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:53:45,367 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:53:45,368 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:53:45,453 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:53:45,453 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2100/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:53:45,582 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3884, 'grad_norm': 3.328125, 'learning_rate': 1.7424118259180656e-06, 'epoch': 0.68}\n",
      "{'loss': 2.4467, 'grad_norm': 2.71875, 'learning_rate': 1.6827799258626443e-06, 'epoch': 0.68}\n",
      "{'loss': 2.4846, 'grad_norm': 4.59375, 'learning_rate': 1.623664199521853e-06, 'epoch': 0.69}\n",
      "{'loss': 2.4619, 'grad_norm': 1.7734375, 'learning_rate': 1.5651019856554995e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:54:02,660 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4477, 'grad_norm': 2.0, 'learning_rate': 1.5071302734130488e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:54:02,885 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:54:02,886 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:54:03,120 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:54:03,121 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:54:03,210 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:54:03,211 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2200/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:54:03,338 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2100] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6064, 'grad_norm': 2.421875, 'learning_rate': 1.4497856789704844e-06, 'epoch': 0.71}\n",
      "{'loss': 2.5605, 'grad_norm': 2.453125, 'learning_rate': 1.3931044224027468e-06, 'epoch': 0.72}\n",
      "{'loss': 2.4607, 'grad_norm': 4.125, 'learning_rate': 1.3371223048063543e-06, 'epoch': 0.72}\n",
      "{'loss': 2.3538, 'grad_norm': 2.0625, 'learning_rate': 1.2818746856866688e-06, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:54:20,409 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5046, 'grad_norm': 1.8046875, 'learning_rate': 1.2273964606240718e-06, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:54:20,645 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:54:20,647 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:54:20,875 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:54:20,877 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:54:20,966 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:54:20,967 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2300/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:54:21,093 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2200] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5178, 'grad_norm': 2.359375, 'learning_rate': 1.1737220392331643e-06, 'epoch': 0.74}\n",
      "{'loss': 2.502, 'grad_norm': 2.46875, 'learning_rate': 1.1208853234289247e-06, 'epoch': 0.75}\n",
      "{'loss': 2.4659, 'grad_norm': 1.578125, 'learning_rate': 1.0689196860135234e-06, 'epoch': 0.76}\n",
      "{'loss': 2.4089, 'grad_norm': 1.8515625, 'learning_rate': 1.017857949597352e-06, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:54:38,809 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4489, 'grad_norm': 2.015625, 'learning_rate': 9.677323658675594e-07, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:54:39,062 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:54:39,063 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:54:39,312 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:54:39,313 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:54:39,413 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:54:39,414 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2400/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:54:39,538 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2300] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4048, 'grad_norm': 2.875, 'learning_rate': 9.18574595217189e-07, 'epoch': 0.77}\n",
      "{'loss': 2.4408, 'grad_norm': 1.9296875, 'learning_rate': 8.704156867478037e-07, 'epoch': 0.78}\n",
      "{'loss': 2.2163, 'grad_norm': 2.71875, 'learning_rate': 8.232860586582e-07, 'epoch': 0.79}\n",
      "{'loss': 2.4113, 'grad_norm': 1.8515625, 'learning_rate': 7.772154790316295e-07, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:54:56,534 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4757, 'grad_norm': 2.671875, 'learning_rate': 7.322330470336314e-07, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:54:56,778 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:54:56,779 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:54:57,006 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:54:57,007 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:54:57,096 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:54:57,096 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:54:57,219 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2400] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4691, 'grad_norm': 2.5, 'learning_rate': 6.883671745323834e-07, 'epoch': 0.81}\n",
      "{'loss': 2.3801, 'grad_norm': 1.453125, 'learning_rate': 6.456455681531524e-07, 'epoch': 0.81}\n",
      "{'loss': 2.4441, 'grad_norm': 2.359375, 'learning_rate': 6.040952117781954e-07, 'epoch': 0.82}\n",
      "{'loss': 2.3974, 'grad_norm': 2.609375, 'learning_rate': 5.637423495031657e-07, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:55:14,590 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2226, 'grad_norm': 1.59375, 'learning_rate': 5.24612469060774e-07, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:55:14,866 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:55:14,867 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:55:15,213 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:55:15,214 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:55:15,306 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:55:15,307 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2600/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:55:15,433 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4938, 'grad_norm': 2.265625, 'learning_rate': 4.867302857221953e-07, 'epoch': 0.84}\n",
      "{'loss': 2.4004, 'grad_norm': 2.484375, 'learning_rate': 4.501197266863691e-07, 'epoch': 0.84}\n",
      "{'loss': 2.4434, 'grad_norm': 1.703125, 'learning_rate': 4.148039159670722e-07, 'epoch': 0.85}\n",
      "{'loss': 2.3764, 'grad_norm': 1.4921875, 'learning_rate': 3.808051597872925e-07, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:55:32,118 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3822, 'grad_norm': 1.640625, 'learning_rate': 3.481449324901412e-07, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:55:32,373 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:55:32,374 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:55:32,735 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:55:32,737 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:55:32,821 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:55:32,822 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2700/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:55:32,958 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2600] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4647, 'grad_norm': 1.75, 'learning_rate': 3.168438629752002e-07, 'epoch': 0.87}\n",
      "{'loss': 2.4091, 'grad_norm': 1.8203125, 'learning_rate': 2.869217216688622e-07, 'epoch': 0.88}\n",
      "{'loss': 2.3333, 'grad_norm': 1.6484375, 'learning_rate': 2.583974080369103e-07, 'epoch': 0.88}\n",
      "{'loss': 2.4302, 'grad_norm': 2.515625, 'learning_rate': 2.312889386472078e-07, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:55:50,132 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4022, 'grad_norm': 2.03125, 'learning_rate': 2.0561343579004716e-07, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:55:50,438 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:55:50,440 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:55:50,679 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:55:50,681 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:55:50,771 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:55:50,772 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2800/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:55:50,899 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2700] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4839, 'grad_norm': 3.21875, 'learning_rate': 1.8138711666334684e-07, 'epoch': 0.9}\n",
      "{'loss': 2.3147, 'grad_norm': 2.390625, 'learning_rate': 1.586252831295193e-07, 'epoch': 0.91}\n",
      "{'loss': 2.4312, 'grad_norm': 3.171875, 'learning_rate': 1.3734231205048825e-07, 'epoch': 0.92}\n",
      "{'loss': 2.3268, 'grad_norm': 2.515625, 'learning_rate': 1.1755164620695314e-07, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:56:08,362 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3033, 'grad_norm': 2.734375, 'learning_rate': 9.926578580764234e-08, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:56:08,598 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:56:08,599 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:56:08,822 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:56:08,823 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:56:08,905 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-2900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:56:08,906 >> Special tokens file saved in ./checkpoint_dir/checkpoint-2900/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:56:09,036 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2800] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4841, 'grad_norm': 3.078125, 'learning_rate': 8.249628059391251e-08, 'epoch': 0.93}\n",
      "{'loss': 2.2977, 'grad_norm': 3.09375, 'learning_rate': 6.725372254468344e-08, 'epoch': 0.94}\n",
      "{'loss': 2.5331, 'grad_norm': 3.234375, 'learning_rate': 5.3547739186319836e-08, 'epoch': 0.95}\n",
      "{'loss': 2.5446, 'grad_norm': 2.1875, 'learning_rate': 4.138698751167597e-08, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:56:26,053 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4795, 'grad_norm': 1.6640625, 'learning_rate': 3.077914851215585e-08, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:56:26,276 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:56:26,277 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:56:26,510 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:56:26,512 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:56:26,603 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:56:26,604 >> Special tokens file saved in ./checkpoint_dir/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:56:26,730 >> Deleting older checkpoint [checkpoint_dir/checkpoint-2900] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3037, 'grad_norm': 3.796875, 'learning_rate': 2.1730922326233806e-08, 'epoch': 0.97}\n",
      "{'loss': 2.5372, 'grad_norm': 1.765625, 'learning_rate': 1.4248024007502693e-08, 'epoch': 0.97}\n",
      "{'loss': 2.3542, 'grad_norm': 1.609375, 'learning_rate': 8.335179914925329e-09, 'epoch': 0.98}\n",
      "{'loss': 2.4343, 'grad_norm': 2.234375, 'learning_rate': 3.9961247275624446e-09, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:56:43,895 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-3100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3465, 'grad_norm': 1.921875, 'learning_rate': 1.2335990856710001e-09, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 20:56:44,131 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:56:44,132 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:56:44,369 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:56:44,371 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:56:44,453 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-3100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:56:44,454 >> Special tokens file saved in ./checkpoint_dir/checkpoint-3100/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:56:44,581 >> Deleting older checkpoint [checkpoint_dir/checkpoint-3000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2627, 'grad_norm': 2.109375, 'learning_rate': 4.934785965721167e-11, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 20:56:48,946 >> Saving model checkpoint to ./checkpoint_dir/checkpoint-3125\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:56:49,187 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:56:49,188 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 20:56:49,409 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 20:56:49,411 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 20:56:49,502 >> tokenizer config file saved in ./checkpoint_dir/checkpoint-3125/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 20:56:49,503 >> Special tokens file saved in ./checkpoint_dir/checkpoint-3125/special_tokens_map.json\n",
      "[INFO|trainer.py:3570] 2024-07-08 20:56:49,631 >> Deleting older checkpoint [checkpoint_dir/checkpoint-3100] due to args.save_total_limit\n",
      "[INFO|trainer.py:2383] 2024-07-08 20:56:49,659 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 580.1929, 'train_samples_per_second': 21.543, 'train_steps_per_second': 5.386, 'train_loss': 3.0464051943969728, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet retrieves training metrics, logs them for monitoring, and saves them for future reference. It's a common practice in machine learning to log and save metrics to understand and compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               = 16752384GF\n",
      "  train_loss               =     3.0464\n",
      "  train_runtime            = 0:09:40.19\n",
      "  train_samples_per_second =     21.543\n",
      "  train_steps_per_second   =      5.386\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state of the trainer\n",
    "trainer.save_state() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Evaluate the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets the padding side of the tokenizer to 'left', evaluates the model using a trainer, calculates the number of evaluation samples, logs and saves the evaluation metrics. This is a common step in fine-tuning a model like Microsoft Phi3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3788] 2024-07-08 21:16:31,274 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-08 21:16:31,274 >>   Num examples = 3042\n",
      "[INFO|trainer.py:3793] 2024-07-08 21:16:31,274 >>   Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0979fc51dbba439cb6878d3c8d6b239e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =     2.4107\n",
      "  eval_runtime            = 0:00:31.07\n",
      "  eval_samples            =       2538\n",
      "  eval_samples_per_second =     97.883\n",
      "  eval_steps_per_second   =     24.487\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "metrics = trainer.evaluate()\n",
    "metrics[\"eval_samples\"] = len(processed_test_dataset)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now save it locally before proceeding to preparation before upload to HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3478] 2024-07-08 21:18:15,955 >> Saving model checkpoint to ./checkpoint_dir\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 21:18:16,247 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 21:18:16,248 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 21:18:16,467 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 21:18:16,468 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 21:18:16,562 >> tokenizer config file saved in ./checkpoint_dir/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 21:18:16,563 >> Special tokens file saved in ./checkpoint_dir/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(train_conf.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h) Transform the model to GGUF 8-bit quantized format and load it to Hugging Face Hub    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clonage dans 'llama.cpp'...\n",
      "remote: Enumerating objects: 29214, done.\u001b[K\n",
      "remote: Counting objects: 100% (8439/8439), done.\u001b[K\n",
      "remote: Compressing objects: 100% (577/577), done.\u001b[K\n",
      "remote: Total 29214 (delta 8177), reused 7869 (delta 7862), pack-reused 20775\u001b[K\n",
      "Réception d'objets: 100% (29214/29214), 50.89 Mio | 14.83 Mio/s, fait.\n",
      "Résolution des deltas: 100% (21008/21008), fait.\n"
     ]
    }
   ],
   "source": [
    "!cd ../modules/ && git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: numpy~=1.26.4 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from -r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
      "Collecting sentencepiece~=0.2.0 (from -r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2))\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.40.1 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from -r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.42.0)\n",
      "Requirement already satisfied: gguf>=0.1.0 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from -r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from -r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.3)\n",
      "Collecting torch~=2.2.1 (from -r ../modules/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3))\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp311-cp311-linux_x86_64.whl (186.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.15.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from torch~=2.2.1->-r ../modules/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from torch~=2.2.1->-r ../modules/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from torch~=2.2.1->-r ../modules/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from torch~=2.2.1->-r ../modules/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from torch~=2.2.1->-r ../modules/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from jinja2->torch~=2.2.1->-r ../modules/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.40.1->-r ../modules/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages (from sympy->torch~=2.2.1->-r ../modules/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Installing collected packages: sentencepiece, torch\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.99\n",
      "    Uninstalling sentencepiece-0.1.99:\n",
      "      Successfully uninstalled sentencepiece-0.1.99\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.0\n",
      "    Uninstalling torch-2.3.0:\n",
      "      Successfully uninstalled torch-2.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mistral-common 1.2.1 requires sentencepiece==0.1.99, but you have sentencepiece 0.2.0 which is incompatible.\n",
      "vllm-flash-attn 2.5.9 requires torch==2.3.0, but you have torch 2.2.2+cpu which is incompatible.\n",
      "torchvision 0.18.0 requires torch==2.3.0, but you have torch 2.2.2+cpu which is incompatible.\n",
      "xformers 0.0.26.post1 requires torch==2.3.0, but you have torch 2.2.2+cpu which is incompatible.\n",
      "vllm 0.5.1 requires torch==2.3.0, but you have torch 2.2.2+cpu which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sentencepiece-0.2.0 torch-2.2.2+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../modules/llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code fine-tunes the Microsoft Phi3 model using a custom adapter model. It loads the base model and the adapter, then merges them into a new model, which is then unloaded from memory to save resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 21:46:12,253 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 21:46:12,459 >> loading configuration file config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-08 21:46:12,460 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0700000524520874,\n",
      "      1.1200000047683716,\n",
      "      1.149999976158142,\n",
      "      1.4199999570846558,\n",
      "      1.5699999332427979,\n",
      "      1.7999999523162842,\n",
      "      2.129999876022339,\n",
      "      2.129999876022339,\n",
      "      3.009999990463257,\n",
      "      5.910000324249268,\n",
      "      6.950000286102295,\n",
      "      9.070000648498535,\n",
      "      9.930000305175781,\n",
      "      10.710000038146973,\n",
      "      11.130000114440918,\n",
      "      14.609999656677246,\n",
      "      15.409998893737793,\n",
      "      19.809999465942383,\n",
      "      37.279998779296875,\n",
      "      38.279998779296875,\n",
      "      38.599998474121094,\n",
      "      40.12000274658203,\n",
      "      46.20000457763672,\n",
      "      50.940006256103516,\n",
      "      53.66000747680664,\n",
      "      54.9373893737793,\n",
      "      56.89738845825195,\n",
      "      57.28738784790039,\n",
      "      59.98738479614258,\n",
      "      60.86738586425781,\n",
      "      60.887386322021484,\n",
      "      61.71739196777344,\n",
      "      62.91739273071289,\n",
      "      62.957393646240234,\n",
      "      63.41739273071289,\n",
      "      63.8173942565918,\n",
      "      63.83739471435547,\n",
      "      63.897396087646484,\n",
      "      63.93739700317383,\n",
      "      64.06739807128906,\n",
      "      64.11434936523438,\n",
      "      64.12435150146484,\n",
      "      64.15435028076172,\n",
      "      64.19435119628906,\n",
      "      64.24435424804688,\n",
      "      64.57435607910156,\n",
      "      64.69000244140625,\n",
      "      64.76000213623047\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.4000000000000004,\n",
      "      1.5500000000000005,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.25,\n",
      "      2.3999999999999995,\n",
      "      2.4499999999999993,\n",
      "      2.499999999999999,\n",
      "      2.6999999999999984,\n",
      "      2.6999999999999984,\n",
      "      2.7499999999999982,\n",
      "      2.799999999999998,\n",
      "      2.8999999999999977,\n",
      "      3.049999999999997\n",
      "    ],\n",
      "    \"type\": \"longrope\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-08 21:46:12 - WARNING - transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3 - `flash-attention` package not found, consider installing for better performance: /home/franck/Applications/miniconda3/envs/MaximusLLM/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi.\n",
      "2024-07-08 21:46:12 - WARNING - transformers_modules.microsoft.Phi-3-mini-128k-instruct.d548c233192db00165d842bf8edff054bb3212f8.modeling_phi3 - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3556] 2024-07-08 21:46:12,584 >> loading weights file model.safetensors from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1531] 2024-07-08 21:46:12,586 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-08 21:46:12,591 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03debed9d4234609b9234a981e9d69df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4364] 2024-07-08 21:46:13,912 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-08 21:46:13,913 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-128k-instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:955] 2024-07-08 21:46:14,022 >> loading configuration file generation_config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-08 21:46:14,023 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 21:46:14,382 >> loading file tokenizer.model from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 21:46:14,382 >> loading file tokenizer.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 21:46:14,383 >> loading file added_tokens.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 21:46:14,383 >> loading file special_tokens_map.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2161] 2024-07-08 21:46:14,383 >> loading file tokenizer_config.json from cache at /home/franck/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/d548c233192db00165d842bf8edff054bb3212f8/tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-08 21:46:14,417 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "adapter_model_name = \"./checkpoint_dir\"\n",
    "\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, **model_kwargs)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3FlashAttention2(\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm()\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:472] 2024-07-08 21:46:37,438 >> Configuration saved in ./phi3-128k-3b-v0.1/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-08 21:46:37,439 >> Configuration saved in ./phi3-128k-3b-v0.1/generation_config.json\n",
      "[INFO|modeling_utils.py:2698] 2024-07-08 21:47:07,422 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at ./phi3-128k-3b-v0.1/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-08 21:47:07,424 >> tokenizer config file saved in ./phi3-128k-3b-v0.1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-08 21:47:07,424 >> Special tokens file saved in ./phi3-128k-3b-v0.1/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./phi3-128k-3b-v0.1/tokenizer_config.json',\n",
       " './phi3-128k-3b-v0.1/special_tokens_map.json',\n",
       " './phi3-128k-3b-v0.1/tokenizer.model',\n",
       " './phi3-128k-3b-v0.1/added_tokens.json',\n",
       " './phi3-128k-3b-v0.1/tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.save_pretrained(\"./phi3-128k-3b-v0.1\")\n",
    "tokenizer.save_pretrained(\"./phi3-128k-3b-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: phi3-128k-3b-v0.1\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 32000\n",
      "INFO:gguf.vocab:Setting special token type unk to 0\n",
      "INFO:gguf.vocab:Setting special token type pad to 32000\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> F16, shape = {3072, 32064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> F16, shape = {3072, 32064}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_qkv.weight,    torch.bfloat16 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:../models/maximusLLM-phi3-128k-3b-v0.1.gguf: n_tensors = 197, total_size = 7.6G\n",
      "Writing: 100%|███████████████████████████| 7.64G/7.64G [00:14<00:00, 520Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to ../models/maximusLLM-phi3-128k-3b-v0.1.gguf\n"
     ]
    }
   ],
   "source": [
    "!python ../modules/llama.cpp/convert_hf_to_gguf.py phi3-128k-3b-v0.1 \\\n",
    "  --outfile ../models/maximusLLM-phi3-128k-3b-v0.1.gguf \\\n",
    "  --outtype f16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's finish by uploading the model to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi(token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/awels/maximusLLM-3b-128k-gguf', endpoint='https://huggingface.co', repo_type='model', repo_id='awels/maximusLLM-3b-128k-gguf')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"awels/maximusLLM-3b-128k-gguf\"\n",
    "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e51995ae1a46faae56ed7f530bbb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "maximusLLM-phi3-128k-3b-v0.1.gguf:   0%|          | 0.00/7.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/awels/maximusLLM-3b-128k-gguf/commit/f3ec0e77b755d375849a3465632bcffb13d23f2c', commit_message='Upload maximusLLM-phi3-128k-3b-v0.1.gguf with huggingface_hub', commit_description='', oid='f3ec0e77b755d375849a3465632bcffb13d23f2c', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"../models/maximusLLM-phi3-128k-3b-v0.1.gguf\",\n",
    "    path_in_repo=\"maximusLLM-phi3-128k-3b-v0.1.gguf\",\n",
    "    repo_id=model_id,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MaximusLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
